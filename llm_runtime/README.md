# LLM Runtime Layer

This package will wrap local LLM runtimes (Ollama, vLLM, llama.cpp, etc.) and expose a unified chat/tool interface for the DevOps Copilot Platform.
